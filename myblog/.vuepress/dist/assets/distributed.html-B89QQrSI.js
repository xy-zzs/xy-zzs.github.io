import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as s,o as p,c as r,d as t,w as l,a as e,b as i,f as n}from"./app-PDj9KKPF.js";const c={},d=n('<h1 id="分布式导航栏" tabindex="-1"><a class="header-anchor" href="#分布式导航栏"><span>分布式导航栏</span></a></h1><h2 id="cap" tabindex="-1"><a class="header-anchor" href="#cap"><span>CAP</span></a></h2><p><strong>CAP</strong> 也就是 <strong>Consistency（一致性）</strong>、<strong>Availability（可用性）</strong>、<strong>Partition Tolerance（分区容错性）</strong> 这三个单词首字母组合。</p><p><strong>一致性（Consistency）</strong> : 所有节点访问同一份最新或者一致的数据副本</p><p><strong>可用性（Availability）</strong>: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。</p><p><strong>分区容错性（Partition Tolerance）</strong> : 分布式系统出现网络分区的时候，仍然能够对外提供服务。</p><p>分布式系统中，多个节点之前的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫 <strong>网络分区</strong>。</p><p>大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个具有误导性质的说法。其实是 CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C</p><p>因此，<strong>分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。</strong> 比如 ZooKeeper就是 CP 架构，Eureka 就是 AP 架构，Nacos 不仅支持 CP 架构也支持 AP 架构。</p><p><strong>为啥不可能选择 CA 架构呢？</strong> 举个例子：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证 一致性C， 必须要禁止其他节点的读写操作，导致返回失败，这就和 可用性A 发生冲突了。如果为了保证 可用性A，其他节点的读写操作正常的话，那么可能读到数据副本不一致，那就和 C 发生冲突了。</p><p>选择 CP 还是 AP 的关键在于当前的业务场景，没有定论，比如对于需要确保强一致性的场景如银行一般会选择保证 CP 。</p><h2 id="base" tabindex="-1"><a class="header-anchor" href="#base"><span>BASE</span></a></h2><p><strong>BASE</strong> 是 <strong>Basically Available（基本可用）</strong> 、<strong>Soft-state（软状态）</strong> 和 <strong>Eventually Consistent（最终一致性）</strong> 三个短语的缩写。BASE 理论是对 CAP 中一致性 C 和可用性 A 权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理逐步演化而来的，它大大降低了我们对系统的要求。</p><p><mark>BASE 理论的核心思想</mark></p><p>即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。</p><blockquote><p>也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。</p></blockquote><p>**BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充。**因此，AP 方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。</p><p><mark>BASE 理论三要素</mark></p><ol><li>基本可用</li></ol><p>基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。</p><p><strong>什么叫允许损失部分可用性呢？</strong></p><ul><li><strong>响应时间上的损失</strong>: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。</li><li><strong>系统功能上的损失</strong>：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。</li></ul><ol start="2"><li>软状态</li></ol><p>软状态指允许系统中的数据存在中间状态（<strong>CAP 理论中的数据不一致</strong>），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。</p><ol start="3"><li>最终一致性</li></ol><p>最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。</p><blockquote><p>分布式一致性的 3 种级别：</p><ol><li><strong>强一致性</strong> ：系统写入了什么，读出来的就是什么。</li><li><strong>弱一致性</strong> ：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。</li><li><strong>最终一致性</strong> ：弱一致性的升级版，系统会保证在一定时间内达到数据一致的状态。</li></ol><p><strong>业界比较推崇是最终一致性级别，但是某些对数据一致要求十分严格的场景比如银行转账还是要保证强一致性。</strong></p></blockquote><h2 id="分布式id" tabindex="-1"><a class="header-anchor" href="#分布式id"><span>分布式ID</span></a></h2><p>一个最基本的分布式 ID 需要满足下面这些要求：</p><ul><li><strong>全局唯一</strong> ：ID 的全局唯一性肯定是首先要满足的！</li><li><strong>高性能</strong> ： 分布式 ID 的生成速度要快，对本地资源消耗要小。</li><li><strong>高可用</strong> ：生成分布式 ID 的服务要保证可用性无限接近于 100%。</li><li><strong>方便易用</strong> ：拿来即用，使用方便，快速接入！</li><li><strong>有序递增</strong> ：如果要把 ID 存放在数据库的话，ID 的有序性可以提升数据库写入速度。并且，很多时候 ，我们还很有可能会直接通过 ID 来进行排序。</li></ul><p>除了这些之外，一个比较好的分布式 ID 还应保证：</p><ul><li><strong>安全</strong> ：ID 中不包含敏感信息。</li><li><strong>有具体的业务含义</strong> ：生成的 ID 如果能有具体的业务含义，可以让定位问题以及开发更透明化（通过 ID 就能确定是哪个业务）。</li><li><strong>独立部署</strong> ：也就是分布式系统单独有一个发号器服务，专门用来生成分布式 ID。这样就生成 ID 的服务可以和业务相关的服务解耦。不过，这样同样带来了网络调用消耗增加的问题。总的来说，如果需要用到分布式 ID 的场景比较多的话，独立部署的发号器服务还是很有必要的。</li></ul><p><mark>分布式 ID 常见解决方案</mark></p>',33),g=e("p",null,"Redis实现分布式唯一ID主要是通过提供像 INCR 和 INCRBY 这样的自增原子命令，由于Redis自身的单线程的特点所以能保证生成的 ID 肯定是唯一有序的。",-1),m=e("p",null,"但是单机存在性能瓶颈，无法满足高并发的业务需求，所以可以采用集群的方式来实现。集群的方式需要设置分段和步长来实现。",-1),h=e("p",null,"为了避免长期自增后数字过大可以通过与当前时间戳、业务字段组合起来使用。",-1),u=e("p",null,"优点：Redis 实现分布式全局唯一ID，它的性能比较高，生成的数据是有序的，对排序业务有利",-1),f=e("p",null,"缺点：需要系统引进redis组件，增加了系统的配置复杂性；需要编码和配置的工作量比较大；Redis单点故障，影响服务可用性。",-1),b=n('<figure><img src="http://images.xyzzs.top/image/03cb34e0fb7c443f96e222965d85a374.png_char" alt="在这里插入图片描述" tabindex="0" loading="lazy"><figcaption>在这里插入图片描述</figcaption></figure><p>Snowflake ID组成结构：<code>正数位</code>（占1比特）+ <code>时间戳</code>（占41比特）+ <code>机器ID</code>（占5比特）+ <code>数据中心</code>（占5比特）+ <code>自增值</code>（占12比特），总共64比特组成的一个Long类型。</p><p>根据这个算法的逻辑，只需要将这个算法用Java语言实现出来，封装为一个工具方法，那么各个业务应用可以直接使用该工具方法来获取分布式ID，只需保证每个业务应用有自己的工作机器id即可，而不需要单独去搭建一个获取分布式ID的应用。</p><p>优点：</p><ul><li>不依赖外部组件，稳定性高；</li><li>灵活方便，可以根据自身业务特性分配bit位；</li><li>单机上ID单调自增，毫秒数在高位，自增序列在低位，整个ID都是趋势递增的</li></ul><p>缺点：</p><ul><li>强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态</li><li>ID可能不是全局递增。在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，也许有时候也会出现不是全局递增的情况</li></ul>',7),k=n(`<ul><li><p>UidGenerator(百度)</p><p>uid-generator是基于Snowflake算法实现的，与原始的snowflake算法不同在于，uid-generator支持自定义时间戳、工作机器ID和 序列号 等各部分的位数，而且uid-generator中采用用户自定义workId的生成策略。</p><p>uid-generator需要与数据库配合使用，需要新增一个WORKER_NODE表。当应用启动时会向数据库表中去插入一条数据，插入成功后返回的自增ID就是该机器的workId数据，由host，port组成。</p><p>对于uid-generator ID组成结构：</p><p>workId，占用了22个bit位，时间占用了28个bit位，序列化占用了13个bit位，需要注意的是，和原始的snowflake不太一样，时间的单位是秒，而不是毫秒，workId也不一样，而且同一应用每次重启就会消费一个workId。</p></li><li><p>Leaf(美团)</p><p><code>Leaf</code>由美团开发。</p><p><code>Leaf</code>同时支持号段模式和<code>snowflake</code>算法模式，可以切换使用。</p></li><li><p>Tinyid(滴滴)</p><p><code>Tinyid</code>由滴滴开发。</p><p><code>Tinyid</code>提供<code>http</code>和<code>tinyid-client</code>两种方式接入</p></li></ul><h2 id="分布式锁" tabindex="-1"><a class="header-anchor" href="#分布式锁"><span>分布式锁</span></a></h2><p>一个最基本的分布式锁需要满足：</p><ul><li><strong>互斥</strong> ：任意一个时刻，锁只能被一个线程持有；</li><li><strong>高可用</strong> ：锁服务是高可用的。并且，即使客户端的释放锁的代码逻辑出现问题，锁最终一定还是会被释放，不会影响其他线程对共享资源的访问。</li><li><strong>可重入</strong>：一个节点获取了锁之后，还可以再次获取锁。</li></ul><p>最好还有以下性能：</p><ul><li>重试：</li><li>锁自动延时</li><li>主动一致性</li></ul><p>实现分布式锁一般有以下几种方式：</p><h3 id="mysql中的实现" tabindex="-1"><a class="header-anchor" href="#mysql中的实现"><span>MySQL中的实现</span></a></h3><p><mark>悲观锁</mark>——select ...for update</p><p>问题：可能产生死锁问题</p><p><mark>乐观锁</mark>——version版本号 + CAS机制</p><p>问题：并发情况下，自旋重试带来性能开销大</p><p><mark>业务字段</mark></p><p>通过业务具体含义来判断，例如：库存大于0；</p><div class="language-sql line-numbers-mode" data-ext="sql" data-title="sql"><pre class="language-sql"><code><span class="token keyword">update</span> db_stock <span class="token keyword">set</span> count <span class="token operator">=</span> count <span class="token operator">-</span><span class="token number">1</span> 
<span class="token keyword">where</span> count <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">;</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>问题：这是表级锁</p><p><mark>数据库的唯一索引</mark></p><p>获得锁时向表中插入一条记录，释放锁时删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否处于锁定状态。</p><p>存在以下几个问题：</p><ul><li>锁没有失效时间，解锁失败的话其它进程无法再获得该锁；</li><li>只能是非阻塞锁，插入失败直接就报错了，无法重试；</li><li>不可重入，已经获得锁的进程也必须重新获取锁。</li></ul><h3 id="zookeeper-的有序节点" tabindex="-1"><a class="header-anchor" href="#zookeeper-的有序节点"><span>Zookeeper 的有序节点</span></a></h3><p>Zookeeper 提供了一种树形结构的命名空间，/app1/p_1 节点的父节点为 /app1。</p><p>节点类型</p><ul><li>永久节点：不会因为会话结束或者超时而消失；</li><li>临时节点：如果会话结束或者超时就会消失；</li><li>有序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，以此类推</li></ul><p>实现步骤：</p><ul><li>创建一个锁目录 /lock；</li><li>当一个客户端需要获取锁时，在 /lock 下创建临时的且有序的子节点；</li><li>客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁；</li><li>执行业务代码，完成后，删除对应的子节点</li></ul><h3 id="redis-的-setnx-指令" tabindex="-1"><a class="header-anchor" href="#redis-的-setnx-指令"><span>Redis 的 SETNX 指令</span></a></h3><p>使用 SETNX（set if not exist）指令插入一个键值对，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。</p><p>SETNX 指令和数据库的唯一索引类似，保证了只存在一个 Key 的键值对，那么可以用一个 Key 的键值对是否存在来判断是否存于锁定状态。</p><ol><li>独占排他：SETNX</li><li>防死锁：设置过期时间，设置过期时间要用原子方法</li><li>防误删：先判断是否是自己的锁，再删除</li><li>判断和释放的原子性：通过 LUA 脚本</li><li>自动续期：Timer定时器 + LUA 脚本</li></ol><h3 id="redis-的-redlock-算法" tabindex="-1"><a class="header-anchor" href="#redis-的-redlock-算法"><span>Redis 的 RedLock 算法</span></a></h3><p>使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。</p><figure><img src="http://images.xyzzs.top/image/image-20230212190650576.png_char" alt="image-20230212190650576" tabindex="0" loading="lazy"><figcaption>image-20230212190650576</figcaption></figure><ol><li>应用程序获取系统当前时间</li><li>应用程序使用相同的kv值依次从多个redis实例中获取锁。如果某一个节点超过-定时间依然没有获取到锁则直接放弃，尽快尝试从下一个健康的redis节点获取锁，以避免被一个宕机了的节点阻塞</li><li>计算获取锁的消耗时间=客户端程序的系统当前时间-step1中的时间。获取锁的消耗时间小于总的锁定时间 (30s)并且半数以上节点获取锁成功，认为获取锁成功</li><li>计算剩余锁定时间 =总的锁定时间 -step3中的消耗时间</li><li>如果获取锁失败了，对所以Redis节点释放锁。</li></ol><p>以下摘抄自官网：</p><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>In the distributed version of the algorithm we assume we have N Redis masters. Those nodes are totally independent, so we don’t use replication or any other implicit coordination system. We already described how to acquire and release the lock safely in a single instance. We take for granted that the algorithm will use this method to acquire and release the lock in a single instance. In our examples we set N=5, which is a reasonable value, so we need to run 5 Redis masters on different computers or virtual machines in order to ensure that they’ll fail in a mostly independent way.

In order to acquire the lock, the client performs the following operations:

1. It gets the current time in milliseconds.
2. It tries to acquire the lock in all the N instances sequentially, using the same key name and random value in all the instances. During step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client from remaining blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.
3. The client computes how much time elapsed in order to acquire the lock, by subtracting from the current time the timestamp obtained in step 1. If and only if the client was able to acquire the lock in the majority of the instances (at least 3), and the total time elapsed to acquire the lock is less than lock validity time, the lock is considered to be acquired.
4. If the lock was acquired, its validity time is considered to be the initial validity time minus the time elapsed, as computed in step 3.
5. If the client failed to acquire the lock for some reason (either it was not able to lock N/2+1 instances or the validity time is negative), it will try to unlock all the instances (even the instances it believed it was not able to lock).
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>在算法的分布式版本中，我们假设我们有N个Redis master。这些节点是完全独立的，所以我们不使用复制或任何其他隐式协调系统。我们已经描述了如何在单个实例中安全地获取和释放锁。我们想当然地认为，算法将使用这种方法在单个实例中获取和释放锁。在我们的例子中，我们设置N=5，这是一个合理的值，所以我们需要在不同的计算机或虚拟机上运行5个Redis master，以确保它们以独立的方式出现故障。

为了获取锁，客户端执行以下操作:

1. 它得到以毫秒为单位的当前时间。
2. 它尝试按顺序在所有N个实例中获取锁，在所有实例中使用相同的密钥名和随机值。在步骤2中，当在每个实例中设置锁时，客户端使用一个比锁自动释放总时间小的超时来获取锁。例如，如果自动释放时间为10秒，则超时可能在~ 5-50毫秒范围内。这可以防止客户端在尝试与一个宕机的Redis节点通信时长时间处于阻塞状态:如果一个实例不可用，我们应该尽快尝试与下一个实例通信。
3.客户端通过从当前时间减去步骤1中获得的时间戳来计算获取锁所需的时间。当且仅当客户端能够在大多数实例(至少3个)中获得锁，并且获得锁的总时间小于锁的有效时间，则认为该锁已获得。
4. 如果获得了锁，则认为锁的有效时间为初始有效时间减去经过的时间，如步骤3中计算的那样。
5. 如果客户端由于某种原因无法获得锁(要么无法锁定N/2+1个实例，要么有效时间为负)，它将尝试解锁所有实例(即使是它认为自己无法锁定的实例)。

以上翻译结果来自有道神经网络翻译（YNMT）· 通用领域
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p><mark>看门狗机制</mark></p><p>watch dog 在当前节点存活时每10s给分布式锁的key续期 30s；</p><p>watch dog 机制启动，且代码中没有释放锁操作时，watch dog 会不断的给锁续期；</p><p>从可2得出，如果程序释放锁操作时因为异常没有被执行，那么锁无法被释放，所以释放锁操作一定要放到 finally {} 中；</p><p>如果释放锁操作本身异常了，watch dog 不会不停的续期。因为无论释放锁操作是否成功，EXPIRATION_RENEWAL_MAP中的目标 ExpirationEntry 对象已经被移除了，watch dog 通过判断后就不会继续给锁续期了。</p><p>要使 watchLog机制生效 。只要不穿leaseTime即可</p><p>watchlog的延时时间 可以由 lockWatchdogTimeout指定默认延时时间。</p><p>watchdog 会每 lockWatchdogTimeout/3时间，去延时。</p><p>watchdog 通过 类似netty的 Future功能来实现异步延时</p><p>watchdog 最终还是通过 lua脚本来进行延时</p><h2 id="分布式事务" tabindex="-1"><a class="header-anchor" href="#分布式事务"><span>分布式事务</span></a></h2><p>分布式事务在分布式环境下，为了满足可用性、性能与降级服务的需要，降低一致性与隔离性的要求，一方面遵循 BASE 理论。</p><figure><img src="http://images.xyzzs.top/image/image-20230213105222824.png_char" alt="image-20230213105222824" tabindex="0" loading="lazy"><figcaption>image-20230213105222824</figcaption></figure><p>由于分布式事务方案，无法做到完全的ACID的保证，没有一种完美的方案，能够解决掉所有业务问题。因此在实际应用中，会根据业务的不同特性，选择最适合的分布式事务方案。</p><p>❓什么是XA</p><p>XA 规范是 X/Open 组织定义的分布式事务处理( DTP，Distributed Transaction Processing )标准。</p><p>XA 规范主要定义了(全局)事务管理器(TM)和(局部)资源管理器(RM)之间的接口，本地的数据库如mysql在XA中扮演的是RM角色。XA 规范的目的是允许多个资源(如数据库，应用服务器，消息队列等)在同一事务中访问，这样可以使 ACID 属性跨越应用程序而保持有效。</p><p>XA 规范使用两阶段提交(2PC，Two-Phase Commit)来保证所有资源同时提交或回滚任何特定的事务。</p><p>DTP模型定义如下角色:</p><ul><li>AP:即应用程序，可以理解为使用DTP分布式事务的程序</li><li>RM:资源管理器，可以理解为事务的参与者，一般情况下是指一个数据库的实例(MySq)，通过资源管理器对该数据库进行控制，资源管理器控制着分支事务</li><li>TM:事务管理器，负责协调和管理事务，事务管理器控制着全局事务，管理实务生命周期，并协调各个RM。全局事务是指分布式事务处理环境中，需要操作多个数据库共同完成一个工作，这个工作即是一个全局事务。</li></ul><p>DTP模式定义TM和RM之间通讯的接口规范叫XA，简单理解为数据库提供的2PC接口协议，基于数据库的XA协议来实现的2PC又称为XA方案。</p><h3 id="_2pc" tabindex="-1"><a class="header-anchor" href="#_2pc"><span>2PC</span></a></h3><p>2PC，两阶段提交，将事务的提交过程分为资源准备和资源提交两个阶段，并且由事务协调者来协调所有事务参与者，如果准备阶段所有事务参与者都预留资源成功，则进行第二阶段的资源提交，否则事务协调者回滚资源。</p><p>第一阶段（prepare）：由事务协调者询问通知各个事务参与者，是否准备好了执行事务，具体流程图如下：</p><figure><img src="http://images.xyzzs.top/image/d0108a2f18594790ad4d439acb1d87e8.png_char" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>第二阶段 (commit/rollback)：协调者收到各个参与者的准备消息后，根据反馈情况通知各个参与者commit提交或者rollback回滚。</p><p>2PC的缺点：</p><ol><li>性能问题：执行过程中，所有参与节点都是事务阻塞性的，当参与者占有公共资源时，其他第三方节点访问公共资源就不得不处于阻塞状态，为了数据的一致性而牺牲了可用性，对性能影响较大，不适合高并发高性能场景；对资源进行了长时间的锁定，并发度低</li><li>可靠性问题：2PC非常依赖协调者，当协调者发生故障时，尤其是第二阶段，那么所有的参与者就会都处于锁定事务资源的状态中，而无法继续完成事务操作（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）</li><li>数据一致性问题： ：由于网络问题或者TM宕机都有可能会造成数据不一致的情况。比如在第2阶段（提交阶段），部分网络出现问题导致部分参与者收不到 Commit/Rollback 消息的话，就会导致数据不一致。</li><li>二阶段无法解决的问题：协调者在发出 commit 消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了，那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</li></ol><h3 id="_3pc" tabindex="-1"><a class="header-anchor" href="#_3pc"><span>3PC</span></a></h3><p>3PC，三阶段提交协议，是二阶段提交协议的改进版本，三阶段提交有两个改动点：</p>`,67),C=e("li",null,"在第一阶段和第二阶段中插入一个准备阶段，保证了在最后提交阶段之前各参与节点的状态是一致的。",-1),y=n("<p>所以3PC会分为3个阶段，CanCommit 准备阶段、PreCommit 预提交阶段、DoCommit 提交阶段，处理流程如下</p><p><mark>阶段一：CanCommit 准备阶段</mark></p><p>协调者向参与者发送 canCommit 请求，参与者如果可以提交就返回Yes响应，否则返回No响应，具体流程如下：</p><p>（1）事务询问：协调者向所有参与者发出包含事务内容的 canCommit 请求，询问是否可以提交事务，并等待所有参与者答复。 （2）响应反馈：参与者收到 canCommit 请求后，如果认为可以执行事务操作，则反馈 yes 并进入预备状态，否则反馈 no。</p><p><mark>阶段二：PreCommit 阶段</mark></p><p>协调者根据参与者的反应情况来决定是否可以进行事务的 PreCommit 操作。根据响应情况，有以下两种可能：</p><p>执行事务：</p><p>假如所有参与者均反馈 yes，协调者预执行事务，具体如下：</p><p>① 发送预提交请求：协调者向参与者发送 PreCommit 请求，并进入准备阶段 ② 事务预提交 ：参与者接收到 PreCommit 请求后，会执行本地事务操作，并将 undo 和 redo 信息记录到事务日志中（但不提交事务） ③ 响应反馈 ：如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。</p><p>中断事务：</p><p>假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断，流程如下：</p><p>① 发送中断请求 ：协调者向所有参与者发送 abort 请求。 ② 中断事务 ：参与者收到来自协调者的 abort 请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。</p><p><mark>阶段三：doCommit阶段</mark></p><p>该阶段进行真正的事务提交，也可以分为以下两种情况：</p><p>提交事务：</p><p>① 发送提交请求：协调接收到所有参与者发送的ACK响应，那么他将从预提交状态进入到提交状态，并向所有参与者发送 doCommit 请求 ② 本地事务提交：参与者接收到doCommit请求之后，执行正式的事务提交，并在完成事务提交之后释放所有事务资源 ③ 响应反馈：事务提交完之后，向协调者发送ack响应。 ④ 完成事务：协调者接收到所有参与者的ack响应之后，完成事务。</p><p>中断事务：任何一个参与者反馈 no，或者等待超时后协调者尚无法收到所有参与者的反馈，即中断事务</p><p>① 发送中断请求：如果协调者处于工作状态，向所有参与者发出 abort 请求 ② 事务回滚：参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。 ③ 反馈结果：参与者完成事务回滚之后，向协调者反馈ACK消息 ④ 中断事务：协调者接收到参与者反馈的ACK消息之后，执行事务的中断。</p><p>3PC的优缺点：</p>",19),v=e("li",null,[e("p",null,"数据不一致问题依然存在，当在参与者收到 preCommit 请求后等待 doCommit 指令时，此时如果协调者请求中断事务，而协调者因为网络问题无法与参与者正常通信，会导致参与者继续提交事务，造成数据不一致。")],-1),_=n('<blockquote><p>2PC和3PC都无法保证数据绝对的一致性，一般为了预防这种问题，可以添加一个报警，比如监控到事务异常的时候，通过脚本自动补偿差异的信息。</p></blockquote><h3 id="saga" tabindex="-1"><a class="header-anchor" href="#saga"><span>SAGA</span></a></h3><p>Saga 其核心思想是将长事务拆分为多个本地短事务，由Saga事务协调器协调，如果正常结束那就正常完成，如果某个步骤失败，则根据相反顺序一次调用补偿操作。</p><figure><img src="http://images.xyzzs.top/image/image-20230212214232137.png_char" alt="image-20230212214232137" tabindex="0" loading="lazy"><figcaption>image-20230212214232137</figcaption></figure><p>Saga一旦到了Cancel阶段，那么Cancel在业务逻辑上是不允许失败了，理论上补偿一定要成功。如果因为网络或者其他临时故障，导致没有返回成功，那么TM会不断重试，直到Cancel返回成功。</p><p>Saga事务的特点：</p><ul><li>并发度高，不用像XA事务那样长期锁定资源</li><li>需要定义正常操作以及补偿操作，开发量比XA大</li><li>一致性较弱，对于转账，可能发生A用户已扣款，最后转账又失败的情况</li></ul><p>SAGA适用的场景较多，长事务适用，对中间结果不敏感的业务场景适用</p><figure><img src="http://images.xyzzs.top/image/image-20230212214348904.png_char" alt="image-20230212214348904" tabindex="0" loading="lazy"><figcaption>image-20230212214348904</figcaption></figure><h3 id="tcc" tabindex="-1"><a class="header-anchor" href="#tcc"><span>TCC</span></a></h3><p>TCC（Try Confirm Cancel）是应用层的两阶段提交，所以对代码的侵入性强，其核心思想是：针对每个操作，都要实现对应的确认和补偿操作，也就是业务逻辑的每个分支都需要实现 try、confirm、cancel 三个操作，第一阶段由业务代码编排来调用Try接口进行资源预留，当所有参与者的 Try 接口都成功了，事务协调者提交事务，并调用参与者的 confirm 接口真正提交业务操作，否则调用每个参与者的 cancel 接口回滚事务，并且由于 confirm 或者 cancel 有可能会重试，因此对应的部分需要支持幂等。</p><p>TCC分为3个阶段</p><ul><li>Try 阶段：尝试执行，完成所有业务检查（一致性）, 预留必须业务资源（准隔离性）</li><li>Confirm 阶段：确认执行真正执行业务，不作任何业务检查，只使用 Try 阶段预留的业务资源，Confirm 操作要求具备幂等设计，Confirm 失败后需要进行重试。</li><li>Cancel 阶段：取消执行，释放 Try 阶段预留的业务资源。Cancel 阶段的异常和 Confirm 阶段异常处理方案基本上一致，要求满足幂等设计。</li></ul><figure><img src="http://images.xyzzs.top/image/image-20230212213142463.png_char" alt="image-20230212213142463" tabindex="0" loading="lazy"><figcaption>image-20230212213142463</figcaption></figure><p>TCC的Confirm/Cancel阶段在业务逻辑上是不允许返回失败的，如果因为网络或者其他临时故障，导致不能返回成功，TM会不断的重试，直到Confirm/Cancel返回成功。</p><p>TCC 和 2PC/3PC 有什么区别呢？</p><ul><li>2PC/3PC 追求的是强一致性，在两阶段提交的整个过程中，一直会持有数据库的锁。TCC 追求的是最终一致性，不会一直持有各个业务资源的锁。</li><li>2PC/3PC 属于业务代码无侵入的，TCC 对业务代码有侵入。</li><li>2PC/3PC 依靠数据库或者存储资源层面的事务，TCC 主要通过修改业务代码来实现。</li></ul><h3 id="本地消息表" tabindex="-1"><a class="header-anchor" href="#本地消息表"><span>本地消息表</span></a></h3><p>设计核心是将需要分布式处理的任务通过消息的方式来异步确保执行。</p><p>本地消息表这个方案最初是 ebay 架构师 Dan Pritchett 在 2008 年发表给 ACM 的文章。设计核心是将需要分布式处理的任务通过消息的方式来异步确保执行。</p><p>大致流程如下：</p><figure><img src="http://images.xyzzs.top/image/image-20230205202259679.png_char" alt="image-20230205202259679" tabindex="0" loading="lazy"><figcaption>image-20230205202259679</figcaption></figure><p>写本地消息和业务操作放在一个事务里，保证了业务和发消息的原子性，要么他们全都成功，要么全都失败。</p><p>容错机制：</p><ul><li>扣减余额事务 失败时，事务直接回滚，无后续步骤</li><li>轮询生产消息失败， 增加余额事务失败都会进行重试</li></ul><p>本地消息表的特点：</p><ul><li>长事务仅需要分拆成多个任务，使用简单</li><li>生产者需要额外的创建消息表</li><li>每个本地消息表都需要进行轮询</li><li>消费者的逻辑如果无法通过重试成功，那么还需要更多的机制，来回滚操作</li></ul><p>适用于可异步执行的业务，且后续操作无需回滚的业务</p><h3 id="事务消息" tabindex="-1"><a class="header-anchor" href="#事务消息"><span>事务消息</span></a></h3><p>在上述的本地消息表方案中，生产者需要额外创建消息表，还需要对本地消息表进行轮询，业务负担较重。阿里开源的RocketMQ 4.3之后的版本正式支持事务消息，该事务消息本质上是对本地消息表的封装，整体流程与本地消息表一致，把本地消息表放到RocketMQ上，解决生产端的消息发送与本地事务执行的原子性问题。</p><figure><img src="http://images.xyzzs.top/image/c01e11f8061e462baccad1404c82e8d7.png_char" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>事务消息发送及提交：</p><ul><li>发送消息（half消息）</li><li>服务端存储消息，并响应消息的写入结果</li><li>根据发送结果执行本地事务（如果写入失败，此时half消息对业务不可见，本地逻辑不执行）</li><li>根据本地事务状态执行Commit或者Rollback（Commit操作发布消息，消息对消费者可见）</li></ul><p><mark>RocketMQ事务消息</mark></p><p>​ 在本地消息表方案中，保证事务主动方发写业务表数据和写消息表数据的一致性是基于数据库事务，而 RocketMQ 的事务消息相对于普通 MQ提供了 2PC 的提交接口，方案如下：</p><figure><img src="http://images.xyzzs.top/image/033c99659a5349f6bfc9f8afb83709b1.png_char" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>补偿流程：</p><p>对没有Commit/Rollback的事务消息（pending状态的消息），从服务端发起一次&quot;回查&quot;。Producer收到回查消息，返回消息对应的本地事务的状态，为Commit或者Rollbac。事务消息方案与本地消息表机制非常类似，区别主要在于原先相关的本地表操作替换成了一个反查接口</p><p>事务消息特点如下：</p><ul><li>长事务仅需要分拆成多个任务，并提供一个反查接口，使用简单</li><li>消费者的逻辑如果无法通过重试成功，那么还需要更多的机制，来回滚操作</li></ul><p>适用于可异步执行的业务，且后续操作无需回滚的业务</p><h3 id="最大努力通知" tabindex="-1"><a class="header-anchor" href="#最大努力通知"><span>最大努力通知</span></a></h3><p>最大努力通知也称为定期校对，是对MQ事务方案的进一步优化。它在事务主动方增加了消息校对的接口，如果事务被动方没有接收到主动方发送的消息，此时可以调用事务主动方提供的消息校对的接口主动获取。</p><figure><img src="http://images.xyzzs.top/image/0b96a31afd174dceaf295d826cbaa787.png_char" alt="img" tabindex="0" loading="lazy"><figcaption>img</figcaption></figure><p>在可靠消息事务中，事务主动方需要将消息发送出去，并且让接收方成功接收消息，这种可靠性发送是由事务主动方保证的；但是最大努力通知，事务主动方仅仅是尽最大努力（重试，轮询....）将事务发送给事务接收方，所以存在事务被动方接收不到消息的情况，此时需要事务被动方主动调用事务主动方的消息校对接口查询业务消息并消费，这种通知的可靠性是由事务被动方保证的。</p><p>解决方案上，最大努力通知需要：</p><ul><li>提供接口，让接受通知放能够通过接口查询业务处理结果</li><li>消息队列ACK机制，消息队列按照间隔1min、5min、10min、30min、1h、2h、5h、10h的方式，逐步拉大通知间隔 ，直到达到通知要求的时间窗口上限。之后不再通知</li></ul><p>最大努力通知适用于业务通知类型，例如微信交易的结果，就是通过最大努力通知方式通知各个商户，既有回调通知，也有交易查询接口</p><p><mark>各方案常见使用场景总结</mark></p><ul><li>2PC/3PC：依赖于数据库，能够很好的提供强一致性和强事务性，但延迟比较高，比较适合传统的单体应用，在同一个方法中存在跨库操作的情况，不适合高并发和高性能要求的场景。</li><li>TCC：适用于执行时间确定且较短，实时性要求高，对数据一致性要求高，比如互联网金融企业最核心的三个服务：交易、支付、账务。</li><li>本地消息表/MQ 事务：适用于事务中参与方支持操作幂等，对一致性要求不高，业务上能容忍数据不一致到一个人工检查周期，事务涉及的参与方、参与环节较少，业务上有对账/校验系统兜底。</li><li>Saga 事务：由于 Saga 事务不能保证隔离性，需要在业务层控制并发，适合于业务场景事务并发操作同一资源较少的情况。Saga 由于缺少预提交动作，导致补偿动作的实现比较麻烦，例如业务是发送短信，补偿动作则得再发送一次短信说明撤销，用户体验比较差。所以，Saga 事务较适用于补偿动作容易处理的场景</li></ul><h2 id="单体改造成多节点注意事项" tabindex="-1"><a class="header-anchor" href="#单体改造成多节点注意事项"><span>单体改造成多节点注意事项</span></a></h2><ol><li>定时任务</li><li>本地锁问题：synchronized，volatile、AtomicInteger等--&gt;改成分布式锁</li><li>@Transactional事务问题</li><li>本地缓存；map缓存改成redis缓存</li><li>mq重复消费问题。</li><li>http方式调用问题，无法自动负载均衡。</li><li>有些业务会在项目启动的时候就运行，类似定时任务。比如健康检查类，集群情况下每个实例都会运行该类，检查到出错信息，会给钉钉发送重复消息。解决方式：改造成定时任务的形式</li><li>线程池问题，改造成多个节点，之前的线程池参数看情况是否修改</li></ol><h2 id="分布式定时任务技术选型" tabindex="-1"><a class="header-anchor" href="#分布式定时任务技术选型"><span>分布式定时任务技术选型</span></a></h2><p>上面提到的一些定时任务的解决方案都是在单机下执行的，适用于比较简单的定时任务场景比如每天凌晨备份一次数据。</p><p>如果我们需要一些高级特性比如支持任务在分布式场景下的分片和高可用的话，我们就需要用到分布式任务调度框架了。</p><p>通常情况下，一个定时任务的执行往往涉及到下面这些角色：</p><ul><li><strong>任务</strong> ： 首先肯定是要执行的任务，这个任务就是具体的业务逻辑比如定时发送文章。</li><li><strong>调度器</strong> ：其次是调度中心，调度中心主要负责任务管理，会分配任务给执行器。</li><li><strong>执行器</strong> ： 最后就是执行器，执行器接收调度器分派的任务并执行。</li></ul><p><mark>Quartz</mark></p><p>一个很火的开源任务调度框架，完全由<code>Java</code>写成。<code>Quartz</code> 可以说是 Java 定时任务领域的老大哥或者说参考标准，其他的任务调度框架基本都是基于 <code>Quartz</code> 开发的，比如当当网的<code>elastic-job</code>就是基于<code>quartz</code>二次开发之后的分布式调度解决方案。</p><p>使用 <code>Quartz</code> 可以很方便地与 <code>Spring</code> 集成，并且支持动态添加任务和集群。但是，<code>Quartz</code> 使用起来也比较麻烦，API 繁琐。</p><p>并且，<code>Quzrtz</code> 并没有内置 UI 管理控制台</p><p>另外，<code>Quartz</code> 虽然也支持分布式任务。但是，它是在数据库层面，通过数据库的锁机制做的，有非常多的弊端比如系统侵入性严重、节点负载不均衡。有点伪分布式的味道。</p><p><strong>优缺点总结：</strong></p><ul><li>优点： 可以与 <code>Spring</code> 集成，并且支持动态添加任务和集群。</li><li>缺点 ：分布式支持不友好，没有内置 UI 管理控制台、使用麻烦（相比于其他同类型框架来说）</li></ul>',64);function x(A,z){const a=s("font");return p(),r("div",null,[d,t(a,{color:"red"},{default:l(()=>[i("基于Redis")]),_:1}),g,m,h,u,f,t(a,{color:"red"},{default:l(()=>[i("基于雪花算法Snowflake")]),_:1}),b,t(a,{color:"red"},{default:l(()=>[i("开源框架")]),_:1}),k,e("ol",null,[e("li",null,[i("在协调者和参与者中"),t(a,{color:"orchid"},{default:l(()=>[i("引入超时机制")]),_:1})]),C]),y,e("ul",null,[e("li",null,[e("p",null,[i("与2PC相比，3PC降低了阻塞范围，并且"),t(a,{color:"orchid"},{default:l(()=>[i("在等待超时后，协调者或参与者会中断事务")]),_:1}),i("，避免了协调者单点问题，阶段三中协调者出现问题时，参与者会继续提交事务。")])]),v]),_])}const I=o(c,[["render",x],["__file","distributed.html.vue"]]),w=JSON.parse('{"path":"/system-design/distributed.html","title":"分布式导航栏","lang":"zh-CN","frontmatter":{"description":"分布式导航栏 CAP CAP 也就是 Consistency（一致性）、Availability（可用性）、Partition Tolerance（分区容错性） 这三个单词首字母组合。 一致性（Consistency） : 所有节点访问同一份最新或者一致的数据副本 可用性（Availability）: 非故障的节点在合理的时间内返回合理的响应（不是错误...","head":[["meta",{"property":"og:url","content":"https://vuepress-theme-hope-docs-demo.netlify.app/system-design/distributed.html"}],["meta",{"property":"og:site_name","content":"文档演示"}],["meta",{"property":"og:title","content":"分布式导航栏"}],["meta",{"property":"og:description","content":"分布式导航栏 CAP CAP 也就是 Consistency（一致性）、Availability（可用性）、Partition Tolerance（分区容错性） 这三个单词首字母组合。 一致性（Consistency） : 所有节点访问同一份最新或者一致的数据副本 可用性（Availability）: 非故障的节点在合理的时间内返回合理的响应（不是错误..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"http://images.xyzzs.top/image/03cb34e0fb7c443f96e222965d85a374.png_char"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-04-24T15:46:57.000Z"}],["meta",{"property":"article:author","content":"Mr.Hope"}],["meta",{"property":"article:modified_time","content":"2024-04-24T15:46:57.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"分布式导航栏\\",\\"image\\":[\\"http://images.xyzzs.top/image/03cb34e0fb7c443f96e222965d85a374.png_char\\",\\"http://images.xyzzs.top/image/image-20230212190650576.png_char\\",\\"http://images.xyzzs.top/image/image-20230213105222824.png_char\\",\\"http://images.xyzzs.top/image/d0108a2f18594790ad4d439acb1d87e8.png_char\\",\\"http://images.xyzzs.top/image/image-20230212214232137.png_char\\",\\"http://images.xyzzs.top/image/image-20230212214348904.png_char\\",\\"http://images.xyzzs.top/image/image-20230212213142463.png_char\\",\\"http://images.xyzzs.top/image/image-20230205202259679.png_char\\",\\"http://images.xyzzs.top/image/c01e11f8061e462baccad1404c82e8d7.png_char\\",\\"http://images.xyzzs.top/image/033c99659a5349f6bfc9f8afb83709b1.png_char\\",\\"http://images.xyzzs.top/image/0b96a31afd174dceaf295d826cbaa787.png_char\\"],\\"dateModified\\":\\"2024-04-24T15:46:57.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"Mr.Hope\\",\\"url\\":\\"https://mister-hope.com\\"}]}"]]},"headers":[{"level":2,"title":"CAP","slug":"cap","link":"#cap","children":[]},{"level":2,"title":"BASE","slug":"base","link":"#base","children":[]},{"level":2,"title":"分布式ID","slug":"分布式id","link":"#分布式id","children":[]},{"level":2,"title":"分布式锁","slug":"分布式锁","link":"#分布式锁","children":[{"level":3,"title":"MySQL中的实现","slug":"mysql中的实现","link":"#mysql中的实现","children":[]},{"level":3,"title":"Zookeeper 的有序节点","slug":"zookeeper-的有序节点","link":"#zookeeper-的有序节点","children":[]},{"level":3,"title":"Redis 的 SETNX 指令","slug":"redis-的-setnx-指令","link":"#redis-的-setnx-指令","children":[]},{"level":3,"title":"Redis 的 RedLock 算法","slug":"redis-的-redlock-算法","link":"#redis-的-redlock-算法","children":[]}]},{"level":2,"title":"分布式事务","slug":"分布式事务","link":"#分布式事务","children":[{"level":3,"title":"2PC","slug":"_2pc","link":"#_2pc","children":[]},{"level":3,"title":"3PC","slug":"_3pc","link":"#_3pc","children":[]},{"level":3,"title":"SAGA","slug":"saga","link":"#saga","children":[]},{"level":3,"title":"TCC","slug":"tcc","link":"#tcc","children":[]},{"level":3,"title":"本地消息表","slug":"本地消息表","link":"#本地消息表","children":[]},{"level":3,"title":"事务消息","slug":"事务消息","link":"#事务消息","children":[]},{"level":3,"title":"最大努力通知","slug":"最大努力通知","link":"#最大努力通知","children":[]}]},{"level":2,"title":"单体改造成多节点注意事项","slug":"单体改造成多节点注意事项","link":"#单体改造成多节点注意事项","children":[]},{"level":2,"title":"分布式定时任务技术选型","slug":"分布式定时任务技术选型","link":"#分布式定时任务技术选型","children":[]}],"git":{"createdTime":1713973617000,"updatedTime":1713973617000,"contributors":[{"name":"jossezs","email":"zzss5224@163.com","commits":1}]},"readingTime":{"minutes":33.41,"words":10023},"filePathRelative":"system-design/distributed.md","localizedDate":"2024年4月24日","autoDesc":true}');export{I as comp,w as data};
